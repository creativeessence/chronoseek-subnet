# Problem Statement

## The Gap in Video Search

Users often remember *what happens* in a video, but not *when it happens*. 

Existing tools rely on metadata, subtitles, or manual chaptering and cannot reliably answer semantic queries such as:

> "the scene where two generals fight each other with guns"

The **Semantic Video Moment Retrieval (SVMR) Subnet** solves this problem by enabling semantic video moment retrieval in a decentralized, competitive environment.

## Key Challenges

1.  **Metadata Limitations:** Titles and tags don't capture the temporal details of long-form content.
2.  **Labor Intensive:** Manual timestamping (e.g., YouTube chapters) is unscalable for massive archives.
3.  **Semantic Gap:** Keyword search fails on conceptual queries (e.g., searching for "joy" vs. finding a smiling face).
4.  **Centralized Control:** Current advanced search is locked behind "walled gardens" (Google, Apple) with privacy and censorship risks.

## The Solution

A decentralized subnet where miners compete to provide the most accurate timestamp intervals for natural language queries, evaluated by a robust, synthetic ground-truth mechanism.
