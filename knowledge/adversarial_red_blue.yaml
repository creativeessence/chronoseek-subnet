# Pattern: Adversarial Red Team / Blue Team
# Example: AI Video Detection

metadata:
  name: adversarial_red_blue
  category: adversarial
  examples: ["AI Video Detection"]

when_to_use:
  commodity: "detection AND generation capabilities"
  value_source: "arms race improves both sides"
  ground_truth: "expensive or impossible to curate at scale"
  key_benefit: "validators don't control ground truth"

architecture:
  validator:
    phases:
      1: "scrape real content from web (YouTube, social, news)"
      2: "generate text description via video-to-text model"
      3: "send prompt to Red Team miners"
      4: "collect fake + original real"
      5: "shuffle and send both to Blue Team miners"
      6: "score based on detection accuracy and generation quality"
  
  red_team:
    role: generators
    receives: "text prompt describing content"
    produces: "synthetic content matching description"
    goal: "create fakes that evade detection"
    infrastructure: "anonymous endpoints, Chutes, any API"
    transparency: "NOT required (output directly tested)"
  
  blue_team:
    role: detectors
    receives: "pair of content (one real, one fake, shuffled)"
    returns: "P(fake) for each"
    goal: "correctly classify real vs fake"
    transparency: "REQUIRED (open source, auditable)"
    infrastructure: "open source on Chutes, Docker images, inspectable code"

scoring:
  blue_team:
    formula: "(1 - p_real_is_fake) * 0.5 + p_fake_is_fake * 0.5"
    correct_real: "low P(fake) on real"
    correct_fake: "high P(fake) on fake"
  
  red_team:
    formula: "1.0 - avg_detection"
    wins_if: "fake goes undetected"

why_this_works:
  validators_dont_control_truth: "miners generate the fakes"
  no_answer_to_leak: "detection difficulty is emergent"
  arms_race_dynamics: "improves both sides over time"
  fresh_content: "web scraping provides scale and novelty"

fresh_content_via_web:
  source: "entire web at scale"
  benefits:
    - massive_dataset_prevents_memorization
    - timestamps_provide_provenance
    - new_content_constantly_entering
  method: "random scraping with verification"

anti_gaming:
  blue_team_open_source: "prevents hiding leaked info"
  fresh_real_content: "can't pre-memorize"
  red_team_anonymity_ok: "output directly tested"
  multiple_blue_miners: "create consensus signal"

infrastructure_table:
  red_team:
    allowed: "anonymous endpoints, Chutes, any API"
    reason: "output is directly tested"
  blue_team:
    allowed: "open source on Chutes, Docker images, auditable code"
    reason: "must verify not receiving leaked answers"

selection_criteria:
  use_when:
    - detection_generation_task
    - ground_truth_expensive_at_scale
    - want_validator_out_of_trust_equation
    - arms_race_beneficial
  
  dont_use_when:
    - clear_external_ground_truth_available
    - only_one_side_needed
    - adversarial_dynamics_undesirable

key_insight: |
  Validators don't need to know ground truth if miners 
  create the challenges. Red Team creates the challenge,
  Blue Team solves it. The "answer" emerges from competition.
