# Chutes Integration Reference
# SN64 - Decentralized GPU compute for inference

overview:
  what: "Capacity/uptime marketplace for GPU inference"
  provides:
    - decentralized_inference
    - gpu_verification_graval
    - tee_support
    - pay_per_use
  
  api_endpoint: "https://api.chutes.ai"
  llm_endpoint: "https://llm.chutes.ai/v1"

use_cases:
  validators_query_llms:
    purpose: "scoring without local GPU"
    benefit:
      - no_expensive_validators
      - consistent_model_behavior
      - low_barrier_to_validate
    
    code: |
      async def query_llm(prompt: str, api_key: str) -> str:
          payload = {
              "model": "deepseek-ai/DeepSeek-V3-0324",
              "messages": [{"role": "user", "content": prompt}],
              "max_tokens": 1000,
              "temperature": 0.0
          }
          async with aiohttp.ClientSession() as session:
              async with session.post(
                  "https://llm.chutes.ai/v1/chat/completions",
                  headers={"Authorization": f"Bearer {api_key}"},
                  json=payload
              ) as resp:
                  result = await resp.json()
                  return result["choices"][0]["message"]["content"]

  miners_deploy_models:
    purpose: "host fine-tuned models for validator queries"
    commitment: "Chute ID to chain"
    
  tee_private_models:
    purpose: "proprietary models stay encrypted"
    provides:
      - confidentiality
      - attestation
      - integrity

authentication:
  api_key:
    creation: |
      pip install chutes
      chutes register
      chutes keys create --name my-key --admin
    
    types:
      admin: "full access"
      chute_ids: "specific chutes only"
      images: "image management"
  
  epistula_for_validators:
    purpose: "hotkey-based auth for free queries"
    headers:
      - "X-Epistula-Timestamp"
      - "X-Epistula-Signature"
      - "X-Epistula-Hotkey"
      - "X-Epistula-Netuid"

validator_free_access:
  method: "hotkey signature verification"
  config: |
    @Chute(
        validator_free_access={
            "enabled": True,
            "netuid": 123,
            "rate_limit_per_validator": 1000
        }
    )

tee_config:
  purpose: "private model deployment"
  options:
    enabled: true
    attestation_required: true
    encrypt_model_weights: true
  
  verification: |
    async def verify_tee(chute_id: str) -> bool:
        resp = await get(f"https://api.chutes.ai/chutes/{chute_id}/attestation")
        attestation = resp.json()
        return (
            verify_tee_signature(attestation) and
            attestation["timestamp"] > time.time() - 3600 and
            attestation["code_hash"] == EXPECTED_HASH
        )

available_templates:
  vllm: "LLM inference (DeepSeek, Llama, Mistral)"
  sglang: "fast LLM serving"
  diffusion: "image generation (SDXL, Flux)"
  tei: "text embeddings (BGE, E5)"

architecture_patterns:
  pattern_a:
    name: "validators use reference models"
    flow: "validator → Chutes → reference model → score"
    use_case: "text generation scoring"
  
  pattern_b:
    name: "miners deploy to Chutes"
    flow: "validator → Chutes → miner's model"
    use_case: "model training subnets"
  
  pattern_c:
    name: "hybrid"
    flow: "reference for test generation + miner models for evaluation"

cost_mitigation:
  for_validators:
    - use_shared_reference_models
    - batch_requests
    - cache_deterministic_results
    - stake_weighted_sampling
