# Pattern: Data Indexing and Validation
# Example: Data Universe (SN13)

metadata:
  name: data_indexing
  category: data
  examples: ["Data Universe SN13"]

when_to_use:
  commodity: "collected/stored data (scraping, indexing)"
  value_source: "unique, fresh, correctly-formatted data"
  verification: "sample and re-verify portions"

architecture:
  miner:
    operation: "scrape data from sources"
    storage: "decentralized storage bucket (Hippius) or local database"
    
    chain_commitment:
      what: "storage bucket URL committed ONCE to chain"
      constraint: "1 commit per 100 blocks - commit location, NOT data"
      example: "hippius://miner-bucket or https://s3.hippius.com/miner-bucket"
    
    storage_pattern:
      correct: |
        1. Commit bucket URL to chain (once)
        2. Continuously scrape and store data to bucket
        3. Maintain index file in bucket root
        4. Validators pull index and data directly from bucket
      incorrect: |
        # DON'T commit each CID to chain - rate limited!
        for tweet in tweets:
            cid = upload(tweet)
            subtensor.set_commitment(wallet, netuid, cid)  # WRONG
    
    index_format: "CompressedMinerIndex"
    index_contents:
      - data_source: "reddit, twitter, etc"
      - data_label: "subreddit, topic"
      - time_bucket: "hourly/daily"
      - size_metrics: "bytes, count"
    
    endpoints:
      - "GetMinerIndex: return compressed index"
      - "GetDataEntityBucket: return actual data"
    
    provenance:
      method: "Hippius timestamps on stored files"
      proves: "when data was stored without per-item chain commits"
  
  validator:
    phases:
      1_index_query:
        action: "get miner's index summary"
        purpose: "analyze claimed data"
      
      2_content_sampling:
        action: "request specific buckets"
        purpose: "verify format and timestamps"
      
      3_deep_validation:
        action: "re-scrape sample of claimed data"
        purpose: "compare with miner's version"
        output: "match rate"
    
    scoring_components:
      credibility: "EMA of validation success"
      scorable_bytes: "unique data contribution"
      raw_score: "source_weight × scale × time_freshness × scorable_bytes"
      final_score: "raw_score × credibility^2.5"

scorable_bytes_formula:
  formula: "(miner_bytes^2) / total_network_bytes"
  purpose: "penalize duplicated data"
  effect: "unique data gets higher relative score"

anti_gaming:
  deep_validation: "catches fake/fabricated data"
  credibility_ema: "builds slowly, lost quickly"
  scorable_bytes: "penalizes duplication"
  time_freshness: "rewards recent data"

scoring:
  credibility:
    update: "EMA of verification pass/fail"
    multiplier: "credibility^2.5"
    effect: "exponential penalty for failures"
  
  uniqueness:
    formula: "s²/S"
    explanation: "square of miner bytes / total bytes"

selection_criteria:
  use_when:
    - unique_datasets_valued
    - freshness_matters
    - can_rescrape_for_verification
  
  dont_use_when:
    - data_cannot_be_reverified
    - real_time_queries_needed
    - ground_truth_external
